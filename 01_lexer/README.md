## Lexical Analyzer

#### What is lexical analysis and how is it related to other parts in compilation?

In a compiler, the first phase before the compiler can actually start working with the logic of the instructions in a program file is the lexical analysis. The main porpouse of this phase is to read from the input file the characters and in a sequencial way start to give them meaning and asigning the token that better matches every substring of the program. So the result of the phase is a list of tokens with the value of input that matches.

Every programming language defines it's own tokens and rules, that result on a set of words with their own derivation trees. Without this phase we can't do anything with the input and the input has no meaning.

#### How is the lexical structure of the language expressed in the PLY tool? I.e., what parts are needed in the code and how are they related to lexical rules of the language?

The first thing that we need to define for the lexical analysis is the set of tokens that we are going to use in our language, this includes the reservated words that we might have them declared in the set but also in another data structure for easily search them. the next thing that we must do in order to asign a token for each part of the input is the regular expression of each token. This is the main thing that ply is searching in the input file. Finally, we need to write one last regular expression for all the substrings that we are going to ignore in our input.

#### Explain how the following are recognized and handled in your code:

- Keywords

  I created a map with the reserved tokens just like the next snippet and add it to the array of tokens previously created (Only the values).

  ```python
      reserved = {
          'define': 'DEFINE',
          'begin': 'BEGIN',
          'end': 'END',
          'each': 'EACH',
          'select': 'SELECT'
      }
  ```

- Comments

  I define a function to handle the comments, It matches with the strings that starts with the open curly braces and ends with the closing one, no matter what its inside, including the \n for newline. The fucntion just matches with the regex expression but does nothing with the value of the matching substring of the file.

  ```python
      def t_COMMENT(t):
         r'(?s){.*?}'
         pass
  ```

- Whitespace between tokens

  I use the especial variable t_ignore and this was kind of challenging because most probably the new spaces change between OS and in this case the statement ' \t' wasn't really useful to me so I manage to add the \r.


     ```python
         t_ignore = ' \t\r'
     ```

- Operators & delimiters (<-, parenthesis, etc.)

  I define a variable with every operator and add them into the tokens array. Using the ply guidelines I named the expressions t\_[name] and for all the operators and delimiters it was not necessary to create a function that handles something more complex than the literal characters.

  ```python
      # one and two letter tokens:
      t_LARROW = r'<-'
      t_RARROW = r'->'
      t_LPAREN = r'\('
      t_RPAREN = r'\)'
      t_LSQUARE = '\['
      t_RSQUARE = '\]'
      t_COMMA = r','
      t_DOT = r'\.'
      t_PIPE = r'\|'
      t_DOUBLEPLUS = r'\+\+'
      t_DOUBLEMULT = r'\*\*'
      t_DOUBLEDOT = r'\.\.'
      t_COLON = r':'

      t_EQ = r'='
      t_NOTEQ = r'!='
      t_LT = r'<'
      t_LTEQ = r'<='
      t_GT = r'>'
      t_GTEQ = r'>='
      t_PLUS = r'\+'
      t_MINUS = r'-'
      t_MULT = r'\*'
      t_DIV = r'\/'
      t_MOD = r'%'
  ```

* Integer literals

  I declare a function that manages all the digits and then parse them into an integer.

  ```python
  def t_NUMBER_LITERAL(t):
    r'\d+'
    t.value = int(t.value)
    return t
  ```

- String literals

  I declare a function that matches with all string between double quotes, then I cut the string to only pass the actual value of the string, without the quotes.

  ```python
    def t_STRING_LITERAL(t):
      r'\".*?\"'
      t.value = t.value[1:len(t.value) - 1]
      return t
  ```

- Function names

  I made a regex expression that handles all strings that start with an Uppercase letters and at least have one other character, this means that at least must have to characters. And is different from the constants because the functions only have lowercase letters after the first Uppercase letter.

  ```python
    t_funcIDENT = r'[A-Z][a-z0-9_]+'
  ```

- Tuple names

  The tuples is a expression that has lowercase letters between the two arrow characters, but also can be only the arrows.

  ```python
    t_tupleIDENT = r'<[a-z]*>'
  ```

#### How can the lexer distinguish between the following lexical elements:

- Function names & constant names

  Because the constant names are limited to the range of capital letters [A-Z], at least must have one letter in Uppercase. The difference between a function is that a function actually has to have at least a minimum of two characters, the first one has to be a Uppercase letter and the second one a lowercase character. From there, the user has the freedom of choose the from the range of lowercase letters, underscore and digits

- Keywords & variable names

  Actually the lexer has a function that catches all the strings that start with lowercase, so in the function we manually check if exist that particular string in the dictionary of reserved words. If the word matches a reserved word, the program change the type to the reserved word.

- Operators - (minus) & -> (right arrow)

  The lexic analyzer of PLY follows the rule of hierachy of the longest lexem rule. In this case for avoiding always match with the '-' of the '->'

- String literals & variables names

The main difference that the two regex expressions had is that the String literal must be between double quotes and because of the same principle of the longest lexeme, ply matches string literals without the problem of thinking is a variable.

- Comments & other code

  For the comments I only create a separate function that acts as an t_ignore, just match the comment and dosenÂ´t do anything with the value. The mainly thing for matching with the comment is that whatever that is inside of the curly braces is ignored so if it's not closed they might cause some problems.

- Tuple names & two variables compared to each other with <.

  Because for matching the rule of the tuples is obligatory to have the closing > in order to be

#### Did you implement any extras? If so explain them (what and how)

I implement the multiple lines comments, this was easy just adding '(?s)' this statement before the curly braces part makes the single line mode ative and means also that the dot '.' its also going to match with linebreaks, this enables the feature of having multiple line comments.

```python
def t_COMMENT(t):
    r'(?s){.*?}'
    pass
```

#### What did you think of this assignment? What was difficult? What was easy? Did you learn anything useful?

I thing is not as complex as I tought, but has his own difficulty and also is a great exercise for learning and getting use to regular expressions. Personally, I had never learn well to use and create regular expresions, but ply was a great library for helping us to don't need to write tons of code matching and optimizing the lexer and that is a great advantage.
